{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agustin90-ush/langchain-ask-pdf/blob/main/Copy_of_LangChain_Working_with_LLM_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace\n",
        "\n",
        "Hugging face provide two wrappers for LLM. \n",
        "1. Models hosted on HuggingFace Hub via API\n",
        "2. Local pipelines (download models locally). \n",
        "\n",
        "Both works with: text2text-generation, text-generation\n",
        "\n"
      ],
      "metadata": {
        "id": "dpL2NjDCW4KH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9fJuC2tG0Cq"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain huggingface_hub transformers sentence_transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'HUGGINGFACEHUB_API_TOKEN'"
      ],
      "metadata": {
        "id": "2PpqSc3cG7jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the HuggingFaceHub via API\n",
        "- T5 Encoder-Decoder Model"
      ],
      "metadata": {
        "id": "UhauDrynY0cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "Derb_0t-ZESh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, \n",
        "                     llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", \n",
        "                                        model_kwargs={\"temperature\":0, \n",
        "                                                      \"max_length\":64}))"
      ],
      "metadata": {
        "id": "lvO31GCGKhHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the capital of England?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "uWIc38V4t5gA",
        "outputId": "c502a754-20eb-4fab-bcac-b4befc15e340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "London is the capital of England. The final answer: London.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Area 51 famous for?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK61DssCdCmr",
        "outputId": "13f5d918-6b5a-4bcf-8c51-065e84b08e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area 51 is famous for being the location of the US government's secret space program. The US government has been constructing a space station in Area 51. The final answer: space station.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decoder Only Models\n",
        "text-generation models"
      ],
      "metadata": {
        "id": "xsf3NSlXd_52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# llm_chain = LLMChain(prompt=prompt, \n",
        "#                      llm=HuggingFaceHub(repo_id=\"EleutherAI/gpt-j-6b\", \n",
        "#                                         model_kwargs={\"temperature\":0.1, \n",
        "#                                                       \"max_length\":100}))"
      ],
      "metadata": {
        "id": "7jjxbd5HdJrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm_chain = LLMChain(prompt=prompt, \n",
        "#                      llm=HuggingFaceHub(repo_id=\"stabilityai/stablelm-tuned-alpha-7b\", \n",
        "#                                         model_kwargs={\"temperature\":0, \n",
        "#                                                       \"max_length\":100}))\n",
        "\n",
        "# question = \"What is Area 51 famous for?\"\n",
        "\n",
        "# print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "yPmVE5-ujxuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local pipelines (download models locally)\n"
      ],
      "metadata": {
        "id": "-xUmcqkUf9Dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- T5-Flan - Encoder-Decoder (Seq2seq model)"
      ],
      "metadata": {
        "id": "WXOZ_Un6e1oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id = 'google/flan-t5-small'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto')\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipeline)\n"
      ],
      "metadata": {
        "id": "GMg2xiRnfm21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197e912a-7414-4111-8992-6343463b0954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('//172.28.0.1'), PosixPath('http')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-sqzkm6hvn1hf --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(local_llm('What is the capital of England? '))"
      ],
      "metadata": {
        "id": "LvsLFQxehfZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, \n",
        "                     llm=local_llm\n",
        "                     )\n",
        "\n",
        "question = \"What is the capital of England?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UVVbwLegmU8",
        "outputId": "3df538e3-6d36-40de-9523-fc7275f2e5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "England is the capital of England. The capital of England is London. So, the answer is London.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Only Model - Usage\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "microsoft/DialoGPT-large"
      ],
      "metadata": {
        "id": "ccAHJARfnKSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\", \n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        "    max_length=100\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipeline)"
      ],
      "metadata": {
        "id": "Lm9alUvGLEZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}